{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Syncnet DEMO\n",
    "\n",
    "https://github.com/voletiv/syncnet-in-keras/\n",
    "\n",
    "1. Given a video (*.mp4), convert to proper input format to the Syncnet lip & audio models\n",
    "2. Load the Syncnet lip and audio models\n",
    "3. Calculate lip-video and audio embeddings using Syncnet\n",
    "4. Calculate Euclidian distance between the lip and audio embeddings to check if video/audio are in sync"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import dlib\n",
    "import numpy as np\n",
    "import os\n",
    "import scipy.io.wavfile as wav\n",
    "import speechpy\n",
    "\n",
    "import syncnet_params\n",
    "\n",
    "from syncnet_functions import detect_mouth_in_frame, load_pretrained_syncnet_model\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FUNCTIONS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## get_syncnet_lip_model_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_syncnet_lip_model_input(video, shape_predictor_path=\"shape_predictor_68_face_landmarks.dat\"):\n",
    "\n",
    "    detector = dlib.get_frontal_face_detector()\n",
    "    predictor = dlib.shape_predictor(shape_predictor_path)\n",
    "\n",
    "    cap         = cv2.VideoCapture(video)\n",
    "    frameFPS    = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "    frameCount  = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    frameWidth  = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    frameHeight = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "\n",
    "    print(\"FPS: {}\".format(frameFPS))\n",
    "    print(\"Frames: {}\".format(frameCount))\n",
    "    print(\"Width: {}\".format(frameWidth))\n",
    "    print(\"Height: {}\".format(frameHeight))\n",
    "\n",
    "    # Default face rect\n",
    "    face = dlib.rectangle(30, 30, 220, 220)\n",
    "\n",
    "    lip_model_input = []\n",
    "\n",
    "    frame_index = 0\n",
    "\n",
    "    # Read frames from the video\n",
    "    while(cap.isOpened()):\n",
    "\n",
    "        frames = []\n",
    "        copy_over_frame = []\n",
    "        \n",
    "        \n",
    "        for i in range(5):\n",
    "        \n",
    "            _, frame = cap.read()\n",
    "\n",
    "            # If no frame is read, break\n",
    "            if frame is None:\n",
    "                break\n",
    "            mouth, _ = detect_mouth_in_frame(frame, detector, predictor,\n",
    "                                             prevFace=face, verbose=False)\n",
    "            mouth = cv2.cvtColor(mouth, cv2.COLOR_BGR2GRAY)\n",
    "            \n",
    "            try:\n",
    "                mouth = cv2.resize(mouth, (syncnet_params.MOUTH_W, syncnet_params.MOUTH_H))\n",
    "                mouth = mouth - 110.\n",
    "            except:\n",
    "                return\n",
    "\n",
    "            # Subtract 110 from all mouth values (Checked in syncnet_demo.m)\n",
    "           \n",
    "            frames.append(mouth)\n",
    "            frame_index += 1\n",
    "        \n",
    "        if len(frames) == 5:\n",
    "            stacked = np.stack(frames, axis=-1) #syncnet requires (112,112,5)\n",
    "            lip_model_input.append(stacked)\n",
    "        else:\n",
    "            break\n",
    "    \n",
    "    \n",
    "    \n",
    "    return np.array(lip_model_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## get_syncnet_audio_model_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extract_syncnet_mfcc(wav_file, verbose=False):\n",
    "    \"\"\"To extract mfcc features of audio clips 0.2 seconds in length each,\n",
    "    i.e. of 20 MFCC features in each clip (acc. to syncnet paper)\n",
    "    Output mfcc_clips shape === (N, 12, 20, 1),\n",
    "    where N = len(mfcc_features) // 20\n",
    "    \"\"\"\n",
    "\n",
    "    rate, sig = wav.read(wav_file)\n",
    "    if verbose:\n",
    "        print(\"Sig length: {}, sample_rate: {}\".format(len(sig), rate))\n",
    "\n",
    "    try:\n",
    "        mfcc_features = speechpy.feature.mfcc(sig, sampling_frequency=rate, frame_length=0.010, frame_stride=0.010)\n",
    "    except IndexError:\n",
    "        raise ValueError(\"ERROR: Index error occurred while extracting mfcc\")\n",
    "\n",
    "    if verbose:\n",
    "        print(\"mfcc_features shape:\", mfcc_features.shape)\n",
    "\n",
    "    # Number of audio clips = len(mfcc_features) // length of each audio clip\n",
    "    number_of_audio_clips = len(mfcc_features) // syncnet_params.AUDIO_TIME_STEPS\n",
    "\n",
    "    if verbose:\n",
    "        print(\"Number of audio clips:\", number_of_audio_clips)\n",
    "\n",
    "    # Don't consider the first MFCC feature, only consider the next 12 (Checked in syncnet_demo.m)\n",
    "    # Also, only consider syncnet_params.AUDIO_TIME_STEPS*number_of_audio_clips features\n",
    "    mfcc_features = mfcc_features[:syncnet_params.AUDIO_TIME_STEPS*number_of_audio_clips, 1:]\n",
    "\n",
    "    # Reshape mfcc_features from (x, 12) to (x//20, 12, 20, 1)\n",
    "    mfcc_features = np.expand_dims(np.transpose(np.split(mfcc_features, number_of_audio_clips), (0, 2, 1)), axis=-1)\n",
    "\n",
    "    if verbose:\n",
    "        print(\"Final mfcc_features shape:\", mfcc_features.shape)\n",
    "\n",
    "    return mfcc_features\n",
    "\n",
    "\n",
    "def get_syncnet_audio_model_input(video):\n",
    "\n",
    "    # Convert video's audio to .wav file\n",
    "    audio_out = \"{}.wav\".format(video)\n",
    "    command = \"ffmpeg -y -loglevel panic -i {} -acodec pcm_s16le -ac 1 -ar 16000 {}\".format(video, audio_out)\n",
    "    os.system(command)\n",
    "\n",
    "    # Extract proper input to syncnet_audio_model\n",
    "    return extract_syncnet_mfcc(audio_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Given a video, convert to proper inputs to the Syncnet lip & audio models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - Make sure video is of 25fps!\n",
    "If not, use the following ffmpeg command to convert fps:\n",
    "\n",
    "```\n",
    "ffmpeg -i <video>.mp4 -r 25 -y <video_at_25_fps>.mp4\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def convert_video_to_25_fps(video):\n",
    "    cmd = \"ffmpeg -i {} -r 25 -y tmp.mp4\".format(video)\n",
    "    os.system(cmd)\n",
    "    cmd = \"mv tmp.mp4 {}\".format(video)\n",
    "    os.system(cmd)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "video_to_test = \"test/unsynced.mp4\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "convert_video_to_25_fps(video_to_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - Use dlib's landmarks predictor to extract mouth from frames\n",
    "\n",
    "shape_predictor_68_face_landmarks.dat can be downloaded from http://dlib.net/files/shape_predictor_68_face_landmarks.dat.bz2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "shape_predictor_path = \"shape_predictor_68_face_landmarks.dat\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert video to Syncnet lip model input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FPS: 25\n",
      "Frames: 184\n",
      "Width: 320\n",
      "Height: 240\n",
      "(36, 112, 112, 5)\n"
     ]
    }
   ],
   "source": [
    "lip_input = get_syncnet_lip_model_input(video_to_test, shape_predictor_path)\n",
    "\n",
    "if(lip_input is None):\n",
    "    print(\"Cannot detect mouth\")\n",
    "\n",
    "print(lip_input.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert video's audio to Syncnet audio model input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(36, 12, 20, 1)\n"
     ]
    }
   ],
   "source": [
    "audio_input = get_syncnet_audio_model_input(video_to_test)\n",
    "print(audio_input.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Load the Syncnet lip and audio models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "version = 'v4'\n",
    "mode = 'both'\n",
    "syncnet_audio_model, syncnet_lip_model = load_pretrained_syncnet_model(version=version, mode=mode, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# print(syncnet_audio_model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# print(syncnet_lip_model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Calculate lip-video and audio embeddings using Syncnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(36, 128)\n"
     ]
    }
   ],
   "source": [
    "audio_embeddings = syncnet_audio_model.predict(audio_input)\n",
    "print(audio_embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(36, 128)\n"
     ]
    }
   ],
   "source": [
    "lip_embeddings = syncnet_lip_model.predict(lip_input)\n",
    "print(lip_embeddings.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Calculate Euclidian distance between the lip and audio embeddings to check if video/audio are in sync\n",
    "\n",
    "1. Pass the audio frame through the audio model to get its encoding (a 128-dimensional feature), pass the video frame through the lip model to get its encoding (a 128-dimensional features)\n",
    "\n",
    "2. Check the euclidean distance between the audio encoding and the video encoding.\n",
    "\n",
    "3. If the distance is greater than a threshold, then it is said that audio frame and that video frame are not in sync."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def euclidian_distance(np_data_1, np_data_2): \n",
    "    dist = np.sqrt( np.sum(np.square(np_data_1 - np_data_2), axis=-1) )\n",
    "    return dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 10.32788658  10.93498707  11.16175365  15.21106625  17.3611145\n",
      "  13.61974335  11.35865116  11.46149254  11.10038757  11.94046116\n",
      "  14.11838627  11.26506233  12.90506268  10.66808891  12.27015018\n",
      "  11.82931709   5.69952869   5.92957306   7.05595493  13.42576981\n",
      "   9.61630249  13.71674252  12.09910488  12.35336304  11.62494755\n",
      "  12.79245758  13.50096512  12.58195877  13.35623264  13.16633606\n",
      "  14.34947872  16.96587372   8.58695889  10.88697147   9.89152241\n",
      "   9.37435532]\n"
     ]
    }
   ],
   "source": [
    "distance = euclidian_distance(audio_embeddings, lip_embeddings)\n",
    "print(distance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def euclidian_distance_F(np_data_1, np_data_2): \n",
    "\n",
    "    if( np_data_1.shape != np_data_2.shape):\n",
    "        print(\"==> Dimensions don't match {} {}. Clipping\".format(np_data_1.shape, np_data_2.shape))\n",
    "        min_dim = min(np_data_1.shape[0],  np_data_2.shape[0])\n",
    "\n",
    "        np_data_1 = np_data_1[:min_dim,:]\n",
    "        np_data_2 = np_data_2[:min_dim,:]\n",
    "        \n",
    "    dist = np.linalg.norm(np_data_1-np_data_2)\n",
    "    return dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "72.336\n"
     ]
    }
   ],
   "source": [
    "distance_F = euclidian_distance_F(audio_embeddings, lip_embeddings)\n",
    "print(distance_F)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def syncnet_run(video):\n",
    "    print(\"*\" * 30)\n",
    "    print(video)\n",
    "    print(\"*\" * 30)\n",
    "    convert_video_to_25_fps(video)\n",
    "\n",
    "    lip_input = get_syncnet_lip_model_input(video, shape_predictor_path)\n",
    "    if(lip_input is None):\n",
    "        print(\"Can't detect mouth:\")\n",
    "        return\n",
    "    print(\"Lip Input Shape: {}\".format(lip_input.shape))\n",
    "    \n",
    "    audio_input = get_syncnet_audio_model_input(video)\n",
    "    print(\"Audio Input Shape: {}\".format(audio_input.shape))\n",
    "\n",
    "    audio_embeddings = syncnet_audio_model.predict(audio_input)\n",
    "    print(\"Audio Embedding Shape: {}\".format(audio_embeddings.shape))\n",
    "\n",
    "    lip_embeddings = syncnet_lip_model.predict(lip_input)\n",
    "    print(\"Lip Embedding Shape: {}\".format(lip_embeddings.shape))\n",
    "\n",
    "    distance_float = euclidian_distance_F(audio_embeddings, lip_embeddings)\n",
    "\n",
    "    print(\"Distance: {}\".format(distance_float))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******************************\n",
      "test/unsynced.mp4\n",
      "******************************\n",
      "FPS: 25\n",
      "Frames: 184\n",
      "Width: 320\n",
      "Height: 240\n",
      "Lip Input Shape: (36, 112, 112, 5)\n",
      "Audio Input Shape: (36, 12, 20, 1)\n",
      "Audio Embedding Shape: (36, 128)\n",
      "Lip Embedding Shape: (36, 128)\n",
      "Distance: 72.82254791259766\n",
      "******************************\n",
      "test/synced.mp4\n",
      "******************************\n",
      "FPS: 25\n",
      "Frames: 160\n",
      "Width: 320\n",
      "Height: 240\n",
      "Lip Input Shape: (32, 112, 112, 5)\n",
      "Audio Input Shape: (31, 12, 20, 1)\n",
      "Audio Embedding Shape: (31, 128)\n",
      "Lip Embedding Shape: (32, 128)\n",
      "==> Dimensions don't match (31, 128) (32, 128). Clipping\n",
      "Distance: 66.89258575439453\n",
      "******************************\n",
      "test/bad-dub-01.mp4\n",
      "******************************\n",
      "FPS: 25\n",
      "Frames: 37\n",
      "Width: 320\n",
      "Height: 240\n",
      "Lip Input Shape: (7, 112, 112, 5)\n",
      "Audio Input Shape: (7, 12, 20, 1)\n",
      "Audio Embedding Shape: (7, 128)\n",
      "Lip Embedding Shape: (7, 128)\n",
      "Distance: 26.58800506591797\n",
      "******************************\n",
      "test/bad-dub-02.mp4\n",
      "******************************\n",
      "FPS: 25\n",
      "Frames: 69\n",
      "Width: 320\n",
      "Height: 240\n",
      "Lip Input Shape: (13, 112, 112, 5)\n",
      "Audio Input Shape: (13, 12, 20, 1)\n",
      "Audio Embedding Shape: (13, 128)\n",
      "Lip Embedding Shape: (13, 128)\n",
      "Distance: 35.6924934387207\n",
      "******************************\n",
      "test/bad-dub-03.mp4\n",
      "******************************\n",
      "FPS: 25\n",
      "Frames: 134\n",
      "Width: 320\n",
      "Height: 240\n",
      "Lip Input Shape: (26, 112, 112, 5)\n",
      "Audio Input Shape: (26, 12, 20, 1)\n",
      "Audio Embedding Shape: (26, 128)\n",
      "Lip Embedding Shape: (26, 128)\n",
      "Distance: 57.853736877441406\n",
      "******************************\n",
      "test/bad-dub-04.mp4\n",
      "******************************\n",
      "FPS: 25\n",
      "Frames: 89\n",
      "Width: 320\n",
      "Height: 240\n",
      "Can't detect mouth:\n",
      "******************************\n",
      "test/bad-dub-05.mp4\n",
      "******************************\n",
      "FPS: 25\n",
      "Frames: 42\n",
      "Width: 320\n",
      "Height: 240\n",
      "Lip Input Shape: (8, 112, 112, 5)\n",
      "Audio Input Shape: (8, 12, 20, 1)\n",
      "Audio Embedding Shape: (8, 128)\n",
      "Lip Embedding Shape: (8, 128)\n",
      "Distance: 30.734596252441406\n",
      "******************************\n",
      "test/bad-dub-06.mp4\n",
      "******************************\n",
      "FPS: 25\n",
      "Frames: 107\n",
      "Width: 320\n",
      "Height: 240\n",
      "Lip Input Shape: (21, 112, 112, 5)\n",
      "Audio Input Shape: (21, 12, 20, 1)\n",
      "Audio Embedding Shape: (21, 128)\n",
      "Lip Embedding Shape: (21, 128)\n",
      "Distance: 28.16963768005371\n",
      "******************************\n",
      "test/bad-dub-07.mp4\n",
      "******************************\n",
      "FPS: 25\n",
      "Frames: 119\n",
      "Width: 320\n",
      "Height: 240\n"
     ]
    }
   ],
   "source": [
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "\n",
    "\n",
    "syncnet_run(\"test/unsynced.mp4\")\n",
    "syncnet_run(\"test/synced.mp4\")\n",
    "\n",
    "test_path=\"test/\"\n",
    "for f in listdir(test_path):\n",
    "    tfile=join(test_path, f)\n",
    "    if(isfile(tfile) and f.startswith(\"bad-dub\") and f.endswith(\".mp4\")):\n",
    "        syncnet_run(tfile)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
